# Speech Module
The speech module provides a extendable Speech-To-Text (Speech Recognition) and Text-To-Speech (Speech Synthesis) functionalities to Unity program on Windows Standalone, UWP, and Android platforms.

## Components
The speech module consists of three components: speech recognizers, speech synthesizers, and the `SpeechProvider`. You can implement your own recognizers and synthesizers if needed. All licenses of third-party libraries can be found in `THIRD-PARTY-NOTICES` under the `Third Party Plugins` folder.

### Speech Recognizer
All speech recognizers should implement the `ISpeechRecognizer` interface and realize its `StartRecordingAsync()` and `StopRecordingAsync()` methods, `Language` and `IsApplicable` properties, and `OnRecognitionResultReceived` event. It should also inherits ``MonoBehavior``. There are two pre-implemented instances in the module:
  - `AzureSpeechRecognizer`, which uses the [Azure Congitive Service](https://azure.microsoft.com/en-us/services/cognitive-services/#overview) of Microsoft. It provides two modes: SingleShot and Continuous. In the SingleShot mode, it stops automatically when it detects silence, while in the Continuous mode, user must stop it manually. You needs a subscribtion key and service region to use the Azure service, and of course also internet conection. To use this recognizer, one needs the [SpeechSDK](https://docs.microsoft.com/en-us/azure/cognitive-services/speech-service/quickstarts/setup-platform?pivots=programming-language-csharp&tabs=windows%2Cubuntu%2Cunity%2Cjre%2Cmaven%2Cbrowser%2Cmac%2Cpypi).
  - `NativeSpeechRecognizer`, which is neural-network based and can run offline on the device. It uses the [Vosk](https://alphacephei.com/vosk/index) library. Since it is neural-network based, one must specify neural-network models. They can be downloaded [here](https://alphacephei.com/vosk/models). It is suggested to download the small models, which are typically around 40 to 50MB. The models must be placed under `Assets/StreamingAssets` folder. On the inspector view, you need to specify the path of the model. If the model is placed under the `StreamingAssets` folder, the path is only the name of the model with ".zip" at the end. Basically you can add any language that has a model.
### Speech Synthesizer
All speech synthesizer should implement the `ISpeechSyntheizer` interface and realize its `StartSynthesizingAndSpeakingAsync()` method, `Language`, `IsApplicable` and `OutputForm` properties, and the `OnSynthesisResultReceived` event. The `OutputForm` has two values: `To Speaker` and `As Byte Stream`. Considering some APIs allow developers get the raw byte stream, we suggest you to use `As Byte Stream` if you need a spatial sound setting. In this case, the stream will be converted to an `Audio Clip` and played by an `Audio Source` on the attached `GameObject`. It is useful especially when you develop an agent, since the spatial sound make it more human-like. However, if the API that you want to call don't support this, you can then neglect this property, the `SpeechProvider` would take care of it.

Again, there are two implemented instances:
- `AzureSpeechSynthesizer`, which works similar to the `AzureSpeechRecognizer`.
- `NativeSpeechSynthesizer`, which is an offline synthesizer. For Windows Standalone, it uses the [Microsoft Speech API (SAPI)](https://docs.microsoft.com/en-us/previous-versions/windows/desktop/ee125663(v=vs.85)) through the `interop.speechlib.dll`. For UWP, it uses the `Windows.Media.SpeechSynthesis` API through the `TextToSpeechUWP` script, which is a slightly modified version of the [`TextToSpeech` script of `HoloToolkit` of Microsoft](https://github.com/microsoft/MixedRealityToolkit-Unity/blob/htk_release/Assets/HoloToolkit/Utilities/Scripts/TextToSpeech.cs). For Andorid, it uses the scripts and Android plugins from the GitHub repository [nir-takemi/UnityTTS](https://github.com/nir-takemi/UnityTTS). The native synthesizer only supports English on all platforms.

### Speech Provider
The `SpeechProvider` requires at least one `ISpeechRecognizer` and one `ISpeechSynthesizer` on the same `GameObject`. The ones with higher priorities should be placed on top of other recognizers and synthesizers ON the inspector. It manages the `ISpeechRecognizer` and `ISpeechSynthesizer` and exposes their functionalities to users. So you only need to re-implement your own `ISpeechRecognizer` and `ISpeechSynthesizer` if needed, and don't need to care about the user-interaction aspects for each of them. There maybe also other settings (SerializeField) on the recognizers and synthesizers. In case of the selected recognizer or synthesizer is not applicable by checking their `IsApplicable` property, it would automatically find another applicable one. For synthesizers, it repeats the synthesis for the given text again. However, for recognizers, users must repeat what they said since the audio data is not buffered on the device. Moreover, by settings its properties and subcribing its events, the values would be propagated to all recognizers and synthesizers, so you don't need to set them one by one. 

## What You Should Notice
- Don't subscribe to the events or setting the properties of `SpeechProvider` in `Awake()`, since it might haven't initialized all recognizers and synthesizers due to the execution order of scripts.
- Although some recognizer don't require a manually stop, e.g. the `AzureSpeechRecognizer` on the SingleShot mode, it is still a good choice to add a stop button on the UI and call the `StopRecordingAsync()` method of the `SpeechProvider`. When you implement such a recognizer, you can just leave the `StopRecordingAsync()` method empty.
- If you are quite sure about your use cases and only want to use one recognizer/synthesizer, you can also omit the `SpeechProvider` and directly interact with the recognizer/synthesizer.
- `PrimaryAudioOutputForm` and `Language` properties of `SpeechProvider` may not influence all recognizers or syntheisizers because they may not support them.
- Although the methods for recognizing and synthesizing do have return values, they are not guaranteed to be meaningful. In facts, they are meaningless in most cases and should only be used for `await`. Instead, you should subscribe to the `OnRecognitionResultReceived` and `OnSynthesisResultReceived` events to deal with the results.
- The neural-network models for the `NativeSpeechRecognizer` must be stored in the `StreamingAssets` folder, because they would be decompressed to the `PersistentDataPath` on the first start, so they need to be "as is" after build and shouldn't be compressed by Unity.
- The `NativeSpeechSyntheizer` only works with `Mono` Backend and `.NET 4.x` API compatibility level on Windows Standalone. For Android, it must be built with an API level greater or equal than 21.
- Although some third-party libraries contain plugins for other platforms, e.g. MacOS or IOS, they are removed, but you can still find them on the corresponding websites.